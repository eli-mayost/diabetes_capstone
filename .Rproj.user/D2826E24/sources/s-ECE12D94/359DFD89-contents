---
title: "Capstone project - Diabetes"
author: "Eli F Mayost"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    theme: yeti
  pdf_document: default
always_allow_html: yes
---

```{r setup, include=FALSE}
if(!require("stats")) install.packages("stats")
if(!require("readr")) install.packages("readr")
if(!require("dplyr")) install.packages("dplyr")
if(!require("glue")) install.packages("glue")
if(!require("plotly")) install.packages("plotly")
if(!require("rpart")) install.packages("rpart")
if(!require("randomForest")) install.packages("randomForest")
if(!require("knitr")) install.packages("knitr")
if(!require("gridExtra")) install.packages("gridExtra")
if(!require("caret")) install.packages("caret")
if(!require("ggcorrplot")) install.packages("ggcorrplot")
if(!require("rattle")) install.packages("rattle")
```

<!-- Style for various components -->
<style type="text/css">
.title{
  font-weight: bold;
}
.author {
  margin-top: 50px;
  font-size: 1.1em;
  font-weight: bold;
}
.date {
  font-size: 1.1em;
}
body {
  font-size: 14px;
}
h2 {
  font-size: 1.5em;
  font-weight: bold;
  margin-top: 100px;
  margin-bottom: 50px;
}
h3 {
  font-size: 0.9em;
  font-weight: bold;
  margin-top: 100px;
  margin-bottom: 50px;
}
pre {
  font-size: 0.9em;
  margin-top: 20px;
  margin-bottom: 40px;
  padding: 20px;
}
.table {
  font-size: 0.9em;
  margin-top: 50px;
  margin-bottom: 50px;
}
.plotly {
  margin-top: 50px;
  margin-bottom: 50px;
}
img {
  margin-bottom: 50px;
}
.footnotes {
  font-size: 0.9em;
  margin-top: 20px;
  margin-bottom: 20px;
}
</style>

\newpage
## Introduction
More and more people worlwide are affected by Diabetes.

The number of affected people has been rising almost four folds from 1980 to 2014 [^1].

Diabetes is a major cause of blindness, kidney failure, strokes, heart attacks, and lower limb amputation.

46% of people with diabetes are undiagnosed [^2].

The goal of this project is to develop a machine learning model to try and predict diabetes in women.

[^1]: World Health Organization.
[^2]: diabetes.co.uk.

\newpage
## The data

A population of women who were at least 21 years old, of Pima Indian heritage and living near Phoenix, Arizona, were tested for diabetes according to World Health Organization criteria.

The data is in diabetes.csv: \newline

```{r message=F}
data <- readr::read_csv("diabetes.csv")
```

Checking that we do not have missing values: \newline

```{r}
data %>% is.na() %>% any()
```

The dataset consists of 768 observations, with 9 features [^3] for each observation: \newline

```{r}
data %>% dim()
```

The first 10 observations: \newline

```{r}
data %>% head(10) %>% knitr::kable()
```

We can already see some illogical values (BMI, Glucose, SkinThickness, and BloodPressure at 0...).

As we do not have any NA, it appears that a zero was filled in for missing values (the 8th observation has a Blood Pressure of 0).

\newpage
Let's find out how many in each of the above columns: \newline

```{r}
columns <- data %>% 
  dplyr::select(-Outcome, -Pregnancies, -Insulin, -DiabetesPedigreeFunction) %>% 
  colnames()

for(c in columns){
  glue::glue("{as.character(c)} has {sum(data[as.character(c)] == 0)} zeroes") %>% 
  print()
}
```

<!-- variable to hold number of rows that contain at leat one zero in the selected columns to stay DRY -->
`r total_rows <- data %>% filter_at(vars(BMI, SkinThickness, BloodPressure, Glucose), any_vars(. == 0)) %>% nrow()`

`r total_rows` rows are affected: \newline

```{r}
data %>% dplyr::filter_at(vars(BMI, SkinThickness, BloodPressure, Glucose), any_vars(. == 0)) %>%
  nrow()
```

So the zero values are in the 4 columns previously mentioned.
We could eliminate all those `r total_rows` observations, but we would be loosing an important portion of observations for training our model later, as well as important information contained in those rows.

I will replace the zero values with the mean of the other observations having the same outcome. \newline

```{r}

bmi_mean <- data %>%
  dplyr::filter(! BMI == 0) %>% 
  dplyr::group_by(Outcome) %>% 
  dplyr::summarize(avg = mean(BMI)) %>% 
  .$avg

bp_mean <- data %>% 
  dplyr::filter(! BloodPressure == 0) %>% 
  dplyr::group_by(Outcome) %>% 
  dplyr::summarize(avg = mean(BloodPressure)) %>% 
  .$avg

st_mean <- data %>% 
  dplyr::filter(! SkinThickness == 0) %>% 
  dplyr::group_by(Outcome) %>% 
  dplyr::summarize(avg = mean(SkinThickness)) %>% 
  .$avg

gl_mean <- data %>% 
  dplyr::filter(! Glucose == 0) %>% 
  dplyr::group_by(Outcome) %>% 
  dplyr::summarize(avg = mean(Glucose)) %>% 
  .$avg

data <- data %>%
  dplyr::mutate(BMI = ifelse(BMI == 0 & Outcome == 0, bmi_mean[1],
    ifelse(BMI == 0 & Outcome == 1, bmi_mean[2], BMI))) %>%
  
  dplyr::mutate(BloodPressure = ifelse(BloodPressure == 0 & Outcome == 0, bp_mean[1],
    ifelse(BloodPressure == 0 & Outcome == 1, bp_mean[2], BloodPressure))) %>%
  
  dplyr::mutate(SkinThickness = ifelse(SkinThickness == 0 & Outcome == 0, st_mean[1],
    ifelse(SkinThickness == 0 & Outcome == 1, st_mean[2], SkinThickness))) %>%
  
  dplyr::mutate(Glucose = ifelse(Glucose == 0 & Outcome == 0, gl_mean[1],
    ifelse(Glucose == 0 & Outcome == 1, gl_mean[2], Glucose)))

data %>% head(10) %>% round(digits = 2) %>% knitr::kable()

```

Now, we should not have any zero value, and we keep the same observations' total: \newline

```{r}
data %>%
  dplyr::filter_at(vars(BMI, SkinThickness, Age, BloodPressure), any_vars(. == 0)) %>%
  nrow()
```

[^3]: The 9 features are: Number of pregnancies, plasma glucose concentration,  diastolic blood pressure (mm Hg), triceps skin fold thickness (in mm), 2 hour serum insulin measure, body mass index, diabetes pedigree function, age, and outcome (0 for negative, and 1 for positive diabetes).

\newpage
## Exploring the data

A quick summary of the features: \newline

```{r}
data %>% 
  dplyr::select(Pregnancies, Glucose, BloodPressure, SkinThickness) %>% 
  summary() %>% 
  knitr::kable()

data %>% 
  dplyr::select(Insulin, BMI, Age, DiabetesPedigreeFunction) %>% 
  summary() %>% 
  knitr::kable()
```

For the outcome, total of negative and positive observations: \newline

```{r}
data %>%
  ggplot(aes(Outcome)) + 
  geom_bar(aes(color = factor(Outcome), fill = factor(Outcome)), show.legend = F) + 
  scale_x_discrete(limits = c(0, 1)) + 
  scale_y_discrete(name = "Total", limits = seq(0, sum(data$Outcome == 0), 100)) + 
  scale_color_manual(values = c("steelblue", "purple")) +
  scale_fill_manual(values = c("steelblue", "purple")) +
  theme_minimal() +
  theme(axis.title.x = element_text(size = 10),
        axis.title.y = element_text(size = 10))
```

Exploring the correlation between features and outcome: \newline

```{r}
data %>% 
  dplyr::mutate(Outcome = as.integer(Outcome)) %>% 
  cor %>% 
  round(digits = 2) %>% 
  knitr::kable()
```

There is a positive correlation between all the features and the Outcome (Glucose being the strongest one).

We can visualize the correlations like so: \newline

```{r}
data %>% 
  cor %>% 
  ggcorrplot(type = "lower",
             lab = T, 
             colors = c("#6D9EC1", "white", "#E46726"), 
             outline.color = "darkgrey",
             tl.cex = 10,
             legend.title = "correlation")
```

We can see if the data is grouped, the outliers, and the reliationship betwen a categorical feature (outcome), and 
a continuous one (Glucose, BMI etc...) with boxplots: \newline
  
```{r}
make_plot <- function(feature){
  data %>% 
  ggplot(aes(x=Outcome, y=!!rlang::sym(feature))) + 
    
    geom_boxplot(aes(fill=factor(Outcome))) +
    scale_x_discrete(limits = c(0, 1)) +
    scale_color_manual(values = c("steelblue", "purple")) +
    scale_fill_manual("Outcome", values = c("steelblue", "purple")) +
    theme_minimal() +
    theme(axis.title.x = element_text(size = 10),
          axis.title.y = element_text(size = 10),
          legend.title = element_text(size = 10),
          plot.margin = unit(c(1,1,1,1), "cm"))
}

p1 <- make_plot("BMI")
p2 <- make_plot("Insulin")
p3 <- make_plot("SkinThickness")
p4 <- make_plot("Age")
p5 <- make_plot("BloodPressure")
p6 <- make_plot("DiabetesPedigreeFunction")
p7 <- make_plot("Pregnancies")
p8 <- make_plot("Glucose")

grid.arrange(
  grobs = list(p1, p2, p3, p4),
  layout_matrix = rbind(c(1, 2),
                        c(3, 4))
)

grid.arrange(
  grobs = list(p5, p6, p7, p8),
  layout_matrix = rbind(c(1, 2),
                        c(3, 4))
)
```

For the models I will use all the features, but a possible future improvement could be to keep only BMI, SkinThickness, and Glucose. The features where there is a clear demarcation between the two outcomes, and that contain relatively few outliers.

\newpage
## Preparing train and test sets

We will partition the data in two; 70% for the train set, and 30% for the test set: \newline

```{r}
ind <- caret::createDataPartition(data$Outcome, times = 1, p = 0.3, list = F)

train_set <- data %>% dplyr::slice(-ind)
test_set <- data %>% dplyr::slice(ind)
```

Checking how many observations in each set: \newline

```{r}
glue::glue("The train set has {nrow(train_set)} observations.")
glue::glue("The test set has {nrow(test_set)} observations.")
```

\newpage
## Modelling

From the caret package we will make sure that we are choosing the methods that are fit for classification.

We will see the performance of a few models, and an ensemble of three, in order to choose the best one at the end.

I will try and tune them as we go.

### Guessing

The simplest to try, is guessing: \newline

```{r}
set.seed(1976)

y_hat_guess <- sample(c(0, 1), size = nrow(test_set), replace = T)
conf_matrix_guess <- confusionMatrix(factor(y_hat_guess), factor(test_set$Outcome))
conf_matrix_guess
```

Not great, as we are unable to predict the outcome in roughly one case out of two (`r paste(round(conf_matrix_guess$overall["Accuracy"] * 100, digits = 2), "%", sep = "")`).
That is to be expected, and similar to a coin toss.

\newpage
### Logistic regression

Next we will try logistic regression: \newline

```{r}
set.seed(1976)
fit_glm <- train(factor(Outcome) ~ ., data = train_set, method = "glm")
y_hat_glm <- predict(fit_glm, newdata = test_set)
conf_matrix_glm <- confusionMatrix(factor(y_hat_glm), factor(test_set$Outcome))
conf_matrix_glm
```

Already a sizeable improvement over somply guessing.

\newpage
### Recursive Partitioning

Here, the available parameter for tuning is cp: \newline

```{r}
getModelInfo()$rpart$parameters
```

```{r}
set.seed(1976)

tune_grid_rpart <- expand.grid(cp = seq(0, 0.2, 0.0025))

fit_rpart <- train(factor(Outcome) ~ ., data = train_set, method = "rpart", tuneGrid = tune_grid_rpart)
y_hat_rpart <- predict(fit_rpart, type = "raw", newdata = test_set)
conf_matrix_rpart <- confusionMatrix(factor(y_hat_rpart), factor(test_set$Outcome))
conf_matrix_rpart
```

The best cp value is `r fit_rpart$bestTune `: \newline

```{r}
fit_rpart$results %>% 
  as_tibble() %>% 
  ggplot(aes(x = cp, y = Accuracy)) + 
    geom_line(color = "steelblue") + 
    theme_minimal()
```

We can visualize the final model's decision tree like so: \newline

```{r}
rattle::fancyRpartPlot(fit_rpart$finalModel, caption = "")
```

\newpage
### Random Forest

Here, also only one parameter, mtry, to tune.

Maximum value for mtry is the number of predictors, so 8 in our case. We tune using tuneGrid, and see which one gives the best accuracy. \newline

```{r}
set.seed(1976)
fit_rf <- train(factor(Outcome) ~ .,
             data = train_set, 
             method = "rf",
             tuneGrid = data.frame(mtry = seq(train_set %>% colnames() %>% length() - 1)))
y_hat_rf <- predict(fit_rf, newdata = test_set)
conf_matrix_rf <- confusionMatrix(factor(y_hat_rf), factor(test_set$Outcome))
conf_matrix_rf
```

We can see that we have a high percentage when coming to detect people without diabetes (`r glue::glue(round(conf_matrix_rf$byClass["Sensitivity"]*100, digits = 2), "%")`).
The accuracy is not that high when we try and detect positive outcome with only (`r glue::glue(round(conf_matrix_rf$byClass["Specificity"]*100, digits = 2), "%")`).

The best mtry value is `r fit_rf$bestTune`. This can be seen in the following graph: \newline

```{r}
cols <- replicate(fit_rf$results %>% nrow, "steelblue")
cols[which.max(fit_rf$results$Accuracy)] = "red"

fit_rf$results %>% 
  as_tibble() %>% 
  ggplot(aes(x = mtry, y = Accuracy)) + 
    geom_line(color = "steelblue") + 
    geom_label(aes(label = mtry), fill = cols, color = "white") + 
    theme_minimal()
```

\newpage
### K Nearest Neighbours

k is the tuning parameter. We will try every second k values from 1 to 100. \newline

```{r}
set.seed(1976)
fit_knn <- train(factor(Outcome) ~ ., data = train_set, method = "knn", tuneGrid = data.frame(k = seq(1, 100, 2)))
y_hat_knn <- predict(fit_knn, newdata = test_set)
conf_matrix_knn <- confusionMatrix(factor(y_hat_knn), factor(test_set$Outcome))
conf_matrix_knn
```

We can graph the accuracy vs. the k value like so: \newline

```{r}
cols <- replicate(fit_knn$results %>% nrow, "steelblue")
cols[which.max(fit_knn$results$Accuracy)] = "red"

fit_knn$results %>% 
  as_tibble() %>% 
  ggplot(aes(x = k, y = Accuracy)) + 
    geom_line(color = "steelblue") + 
    geom_label(aes(label = k), fill = cols, color = "white") + 
    theme_minimal()
```

The best k value here is `r fit_knn$bestTune`.

\newpage
### Gradient Boosting Machine

To tune gbm, the following parameters are required: \newline

```{r}
getModelInfo()$gbm$parameters
```

For interaction.depth, the max value is the number of predictors, so `r train_set %>% names() %>% length() - 1` in our case.
For shrinkage, generally, the smaller the number the better the predicting value keeping in mind the modest computational resources of a laptop.
n.minobsinnode's default is 10, but for classification smaller number work well. To minimize the time it runs, we will leave it at 10, but we could supply it a sequence like seq(1, 11, 2).
For small shrinkage, we need more trees. We will go up to 5000 trees. \newline

```{r}
set.seed(1976)

tune_grid_gbm <- expand.grid(n.trees = seq(50, 5000, 50),
                             interaction.depth = seq(1, 9, 2), 
                             shrinkage = 0.01,
                             n.minobsinnode = 10)
fit_gbm <- train(factor(Outcome) ~ ., data = train_set, method = "gbm", tuneGrid = tune_grid_gbm, verbose = F)
y_hat_gbm <- predict(fit_gbm, newdata = test_set)
conf_matrix_gbm <- confusionMatrix(factor(y_hat_gbm), factor(test_set$Outcome))
conf_matrix_gbm
```

\newpage
### ensemble rpart, rf, and gbm

We take an average of the different models' predictions, and declare an outcome of 1 when the average is equal or above 2/3 (either 2 or 3 of the methods precict diabetes). \newline

```{r}
y_hat_ensemble <- (y_hat_rf %>% as.character() %>% as.numeric() + 
                   y_hat_gbm %>% as.character() %>% as.numeric() +
                   y_hat_rpart %>% as.character() %>% as.numeric()) / 3

y_hat_ensemble <-  replace(y_hat_ensemble, y_hat_ensemble >= 2/3, 1)
y_hat_ensemble <-  replace(y_hat_ensemble, y_hat_ensemble < 2/3, 0)

conf_matrix_ensemble <- confusionMatrix(factor(y_hat_ensemble), factor(test_set$Outcome))
conf_matrix_ensemble
```


Let's summarize the accuracies of the different methods: \newline

```{r}
  accuracies <- c(
    conf_matrix_guess$overall["Accuracy"],
    conf_matrix_glm$overall["Accuracy"],
    conf_matrix_rpart$overall["Accuracy"],
    conf_matrix_rf$overall["Accuracy"],
    conf_matrix_knn$overall["Accuracy"],
    conf_matrix_gbm$overall["Accuracy"],
    conf_matrix_ensemble$overall["Accuracy"]
  )

accuracies_table <- accuracies %>%
  as.table() %>%
  setNames(c("guessing", "glm", "rpart", "rf", "knn", "gbm", "ensemble"))

accuracies_table
```

\newpage
## Conclusion

When approaching a choice of a method, in this particular case, we will choose the method that produces the highest overall accuracy as the model will, undoubtedly, be doubled by proper medical (repeated?) detection.

That method is `r accuracies_table %>% which.max() %>% names()` with an accuracy of `r accuracies_table[accuracies_table %>% which.max()] %>% round(digits = 4) %>% paste("%", sep = "")`.

Further improvement in accuracy could probably be gained by removing some features, and by tuning further, althouthg that would come at a price of computational time and resources.



